{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19f4d8-848c-4ebb-aec2-8c7fd75edf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from torchinfo import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "os.environ['TORCH_HOME'] = os.getcwd()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f202fd-4296-40d0-8d30-8a87b1f8ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['Clean_ins',\n",
    "        'DATI',\n",
    "        'OffTrkWrite',\n",
    "        'Scratch_radial',\n",
    "        'Ser_deg_ins',\n",
    "        'Ser_instability',\n",
    "        'Spacing_MD_PW_SerDeg',\n",
    "        'Wr_related_1sct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e64a48-8c62-41d7-84a7-801f46519208",
   "metadata": {},
   "source": [
    "# Segregate training data to train (80%) and validation (20%) data \n",
    "*Each category is equally segregated to **training** and **validation** folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921c349-8b72-4123-b4d0-fdd7f05db837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def consolidate_data(datapaths:list,categories:list):\n",
    "    img_dict = {}\n",
    "    for cat in categories:\n",
    "        img_dict[cat] = []\n",
    "        \n",
    "    dupe_all = []\n",
    "    img_all = []\n",
    "    for datapath in datapaths:\n",
    "        imgpaths = glob(datapath)\n",
    "        img_name = []\n",
    "        dupe_name = []\n",
    "        for imgpath in imgpaths:\n",
    "            if os.path.basename(imgpath) in img_all:\n",
    "                dupe_name.append(imgpath)\n",
    "                continue\n",
    "            no_cat = True\n",
    "            for cat in categories:\n",
    "                if cat in os.path.dirname(imgpath):\n",
    "                    img_dict[cat].append(imgpath)\n",
    "                    no_cat = False\n",
    "            img_name.append(os.path.basename(imgpath))\n",
    "            \n",
    "        dupe_all += dupe_name\n",
    "        img_all += img_name\n",
    "        \n",
    "    return img_dict,img_all,dupe_all\n",
    "\n",
    "def split_train_test(datapaths:list,categories:list,train_ratio:float,dataset:str='both'):\n",
    "    train_dir = './data/training'\n",
    "    val_dir = './data/validation'\n",
    "    \n",
    "    # categories.append('NG')\n",
    "    for cat in categories:\n",
    "        train_path = os.path.join(train_dir,cat)\n",
    "        val_path = os.path.join(val_dir,cat)\n",
    "        os.makedirs(train_path,exist_ok=True)\n",
    "        os.makedirs(val_path,exist_ok=True)\n",
    "        \n",
    "    img_dict,img_all,dupe_all = consolidate_data(datapaths,categories)\n",
    "    # print(len(img_all))\n",
    "    l = 0\n",
    "    for k in img_dict.keys():\n",
    "        # print(len(img_dict[k]))\n",
    "        l += len(img_dict[k])\n",
    "    # print(l)\n",
    "    \n",
    "    print('\\n')\n",
    "    for cat in categories:\n",
    "        cat_img = img_dict[cat]\n",
    "        # print(len(cat_img))\n",
    "        np.random.shuffle(cat_img)\n",
    "        if train_ratio < 1 and dataset == 'both':\n",
    "            train_len = int(len(cat_img) * train_ratio)\n",
    "            train_img = cat_img[:train_len]\n",
    "            val_img = cat_img[train_len:]\n",
    "            # print(len(train_img),len(val_img),len(train_img)+len(val_img))\n",
    "\n",
    "            for imgpath in train_img:\n",
    "                # img = cv2.imread(imgpath)\n",
    "                # cv2.imwrite(os.path.join(train_dir,cat,os.path.basename(imgpath)),img)\n",
    "                shutil.copy(imgpath,os.path.join(train_dir,cat,os.path.basename(imgpath)))\n",
    "                pass\n",
    "\n",
    "            for imgpath in val_img:\n",
    "                # img = cv2.imread(imgpath)\n",
    "                # cv2.imwrite(os.path.join(val_dir,cat,os.path.basename(imgpath)),img)\n",
    "                shutil.copy(imgpath,os.path.join(val_dir,cat,os.path.basename(imgpath)))\n",
    "                pass\n",
    "        elif train_ratio == 1 and (dataset == 'validate' or dataset == 'training'):\n",
    "            if dataset == 'train':\n",
    "                for imgpath in cat_img:\n",
    "                    # img = cv2.imread(imgpath)\n",
    "                    # if not cv2.imwrite(os.path.join(train_dir,cat,os.path.basename(imgpath)),img):\n",
    "                    #     print(\"not saved\")\n",
    "                    shutil.copy(imgpath,os.path.join(train_dir,cat,os.path.basename(imgpath)))\n",
    "            else:\n",
    "                for imgpath in cat_img:\n",
    "                    # img = cv2.imread(imgpath)\n",
    "                    # if not cv2.imwrite(os.path.join(val_dir,cat,os.path.basename(imgpath)),img):\n",
    "                    #     print(\"not saved\")\n",
    "                    shutil.copy(imgpath,os.path.join(val_dir,cat,os.path.basename(imgpath)))\n",
    "        else:\n",
    "            raise Exception(\"train ratio and dataset not tally\")\n",
    "            \n",
    "    return img_all,dupe_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610d90f-1cbe-43cb-bab4-52f4668ba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapaths = [\n",
    "    '../train_8c_crop_4comp/*/*.png'\n",
    "]\n",
    "\n",
    "categories = cats\n",
    "\n",
    "img_dict,img_all,dupe_all = consolidate_data(datapaths,categories)\n",
    "img_all,dupe_all = split_train_test(datapaths,categories,0.8,dataset='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f8250-4355-4520-8fa9-971fb44af066",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb20d2-d702-46e9-8be1-4a666f3ab0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "\n",
    "class_to_label_dict = {\n",
    "    'Clean_ins': 0,\n",
    "    'DATI': 1,\n",
    "    'OffTrkWrite': 2,\n",
    "    'Scratch_radial': 3,\n",
    "    'Ser_deg_ins': 4,\n",
    "    'Ser_instability': 5,\n",
    "    'Spacing_MD_PW_SerDeg': 6,\n",
    "    'Wr_related_1sct': 7\n",
    "}\n",
    "\n",
    "label_to_class_dict = {\n",
    "    0: 'Clean_ins',\n",
    "    1: 'DATI',\n",
    "    2: 'OffTrkWrite',\n",
    "    3: 'Scratch_radial',\n",
    "    4: 'Ser_deg_ins',\n",
    "    5: 'Ser_instability',\n",
    "    6: 'Spacing_MD_PW_SerDeg',\n",
    "    7: 'Wr_related_1sct'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093309c-ab87-4998-a51b-caa9e829ca24",
   "metadata": {},
   "source": [
    "## Apply transfer learning using EfficientNet-B5 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e66642-df22-4840-adde-c1ee2e2bb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "efficient = models.efficientnet_b5(weights=None)\n",
    "efficient = efficient.to(device)\n",
    "base_conv = nn.Sequential(*list(efficient.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c389af-36fb-46fd-8c2f-38ca30b82ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitflipModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BitflipModel2, self).__init__()\n",
    "        self.custom_model = base_conv\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(2048,8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.custom_model(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975209b-e67d-4407-bc96-55462aafb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BitflipModel2()\n",
    "model.to(device)\n",
    "print(summary(model,input_size=(1,3,170,336),device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ca5a6-0d2b-45ac-acff-65be14de9cba",
   "metadata": {},
   "source": [
    "## Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaac2c-65b4-4fa4-a7f7-62b52a0e6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_path, transform):\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        for k in class_to_label_dict.keys():\n",
    "            if k in self.image_path[idx]:\n",
    "                label = class_to_label_dict[k]\n",
    "            \n",
    "        return (image,label)\n",
    "        # return (img,roi,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad541753-2558-4a16-927b-8aa66241f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    'train': transforms.Compose(\n",
    "        [transforms.RandomChoice([\n",
    "             transforms.Compose([\n",
    "                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                 transforms.RandomHorizontalFlip(p=0.5)\n",
    "             ]),\n",
    "             transforms.RandomVerticalFlip(p=0.5),\n",
    "             transforms.RandomHorizontalFlip(p=0.5)\n",
    "         ]),\n",
    "         transforms.RandomAffine(degrees=0,translate=(0,0.1)),\n",
    "         transforms.Resize((170,336)), #145,435, #128,336\n",
    "         transforms.CenterCrop((170,326)),\n",
    "         transforms.ToTensor(),\n",
    "         ]),\n",
    "    'test': transforms.Compose(\n",
    "        [transforms.Resize((170,336)), #145,435, #128,336\n",
    "         transforms.CenterCrop((170,326)),\n",
    "         transforms.ToTensor(),\n",
    "         ])}\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size, phase='train', train_val_split=True, train_ratio=.8):\n",
    "\n",
    "    data = CustomDataset(data_folder,transform_dict[phase])\n",
    "    \n",
    "    if phase == 'train':\n",
    "        if train_val_split:\n",
    "            train_size = int(train_ratio * len(data))\n",
    "            test_size = len(data) - train_size\n",
    "            data_train, data_val = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "            train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "            val_loader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                                                num_workers=4)\n",
    "            return [train_loader, val_loader]\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "            return train_loader\n",
    "    else: \n",
    "        test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "        return test_loader\n",
    "\n",
    "num_batch = 8\n",
    "directory_train = glob('./data/training/*/*.png')\n",
    "directory_test = glob('./data/validation/*/*.png') \n",
    "loader_train = load_data(directory_train, batch_size=num_batch, phase='train', train_val_split=False, train_ratio=0.5)\n",
    "loader_test = load_data(directory_test, batch_size=num_batch, phase='test', train_val_split=False, train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe246ac-2792-49cd-8601-17b608b6d7bf",
   "metadata": {},
   "source": [
    "## Calculate class weight of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd7af2-15fc-4a6d-8edf-938649e75dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "yw = []\n",
    "for d in directory_train:\n",
    "    for k in class_to_label_dict.keys():\n",
    "        if k in d:\n",
    "            label = class_to_label_dict[k]\n",
    "    yw.append(label)\n",
    "    \n",
    "class_weight = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(yw),y=yw)\n",
    "class_weight = torch.tensor(class_weight).float().to(device)\n",
    "class_weight\n",
    "# pos_weight = class_weight[1]/class_weight[0]\n",
    "# pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5944951-cc7b-49e7-8901-e35261d9bb1c",
   "metadata": {},
   "source": [
    "## Define optimizer, loss function, epochs and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1299db7-448a-4a07-9bcb-1140799f9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "schedulerplat = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.9,patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e02dcf-3fd0-482e-86f3-b96e1dd6e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    _,y_prob = torch.max(y_prob,1)\n",
    "    return (y_prob == y_true).float().mean().item()\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # model.apply(set_bn_eval)\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    loss_acc = 0\n",
    "    correct_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # y = y.float()\n",
    "        pred = model(X)\n",
    "        # pred = pred.squeeze(1)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(loss)\n",
    "        correct = get_accuracy(y,pred)\n",
    "        loss_acc += loss.item()\n",
    "        correct_acc += correct\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(len(X))\n",
    "        if batch % 50 == 0:\n",
    "            # print(batch,len(X))\n",
    "            loss, correct,current = loss.item(), correct,batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  correct: {correct:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    loss_acc /= (np.ceil(size/num_batch))\n",
    "    correct_acc /= (np.ceil(size/num_batch))\n",
    "    \n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct_acc):>0.1f}%, Avg loss: {loss_acc:>8f} \\n\")\n",
    "            \n",
    "    return loss_acc\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # y = y.float()\n",
    "            pred = model(X)\n",
    "            # pred = pred.squeeze(1)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            correct += get_accuracy(y,pred)\n",
    "            test_loss += loss\n",
    "            # print(correct)\n",
    "\n",
    "            \n",
    "    test_loss /= (np.ceil(size/num_batch))\n",
    "    correct /= (np.ceil(size/num_batch))\n",
    "    # print(correct)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebf265-fdea-4f9a-8a60-52b50f538224",
   "metadata": {},
   "source": [
    "## Run model training, save model weight to **model_save** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeff9aa-5d69-41b2-96c2-c70936d9d96f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_acc = []\n",
    "test_loss_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(loader_train, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(loader_test, model, loss_fn)\n",
    "    train_loss_acc.append(train_loss)\n",
    "    test_loss_acc.append(test_loss)\n",
    "    # if t >= 170:\n",
    "    schedulerplat.step(test_loss)\n",
    "    os.makedirs('./model_save',exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'./model_save/epoch_{t}.pth')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40b80c-f79d-4f0a-999e-e688f6c93336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_acc)\n",
    "plt.plot(test_loss_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd6f5a-3cc7-43a7-81a6-7df946c13d05",
   "metadata": {},
   "source": [
    "# Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495c966-a15a-4f41-9980-655b32c6f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./model/epoch_116.pth',map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def predModel(x,verbose=False):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "    return pred\n",
    "\n",
    "def arr2tensor(img_arr):\n",
    "    img = Image.fromarray(img_arr)\n",
    "    img = transform_dict['test'](img)\n",
    "    # mean, std = torch.mean(img), torch.std(img)\n",
    "    # img = (img - mean)/std\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def tensor2arr(img_tensor):\n",
    "    # img_tensor = invTrans(img_tensor)\n",
    "    img_tensor = (img_tensor[0,...].cpu().detach().permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd339569-eb3e-4641-901e-d72e405fe079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "imgpaths = glob(f'../test_8c_crop_4comp/**.png') #VL8_Far641_sd_sp\n",
    "np.random.shuffle(imgpaths)\n",
    "imgpath = imgpaths[0]\n",
    "print(imgpath)\n",
    "\n",
    "img = cv2.imread(imgpath)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cv2.resize(img,(336,170)))\n",
    "plt.show()\n",
    "\n",
    "img = Image.fromarray(img)\n",
    "img = transforms.Resize((170,336))(img)\n",
    "img = transforms.CenterCrop((170,326))(img)\n",
    "img = transforms.ToTensor()(img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)\n",
    "\n",
    "pred = predModel(img)\n",
    "pred = nn.Softmax(dim=1)(pred)\n",
    "pred = torch.argmax(pred).item()\n",
    "class_type = label_to_class_dict[pred]\n",
    "print(pred,class_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c46649-18c2-4dcf-84e6-9b5b74aa2034",
   "metadata": {},
   "source": [
    "# Generate test result CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306a1dc-e3be-4272-9e63-1c2bc7faf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['imgpath','pred'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5b695-d4bf-4c59-b135-60797215d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgpaths = glob(f'../test_8c_crop_4comp/*.png')\n",
    "\n",
    "for i,imgpath in enumerate(imgpaths):\n",
    "    \n",
    "    img = Image.open(imgpath).convert('RGB')\n",
    "    img = transforms.Resize((170,336))(img)\n",
    "    img = transforms.CenterCrop((170,326))(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    pred = predModel(img)\n",
    "    pred = nn.Softmax(dim=1)(pred)\n",
    "    pred = torch.argmax(pred).item()\n",
    "    class_type = label_to_class_dict[pred]\n",
    "    # print(pred,truth,class_type,cat,imgpath)\n",
    "    df.loc[i,'imgpath'] = os.path.basename(imgpath)\n",
    "    df.loc[i,'pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c738dfb-3837-4e96-8fe2-146074217aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "df.to_csv('./submission.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8646f-a6f9-4586-985e-adb85c54add1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
