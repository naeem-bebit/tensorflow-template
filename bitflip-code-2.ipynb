{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19f4d8-848c-4ebb-aec2-8c7fd75edf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from torchinfo import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f202fd-4296-40d0-8d30-8a87b1f8ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['Clean_ins',\n",
    "        'DATI',\n",
    "        'OffTrkWrite',\n",
    "        'Scratch_radial',\n",
    "        'Ser_deg_ins',\n",
    "        'Ser_instability',\n",
    "        'Spacing_MD_PW_SerDeg',\n",
    "        'Wr_related_1sct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b19cb-818b-4c57-a592-78081fce84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    # print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    return size_all_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793762e-df42-4527-8caf-e9880e1d48d1",
   "metadata": {},
   "source": [
    "# Segregate training data to train (80%) and validation (20%) data \n",
    "*Each category is equally segregated to **training** and **validation** folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921c349-8b72-4123-b4d0-fdd7f05db837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def consolidate_data(datapaths:list,categories:list):\n",
    "    img_dict = {}\n",
    "    for cat in categories:\n",
    "        img_dict[cat] = []\n",
    "        \n",
    "    dupe_all = []\n",
    "    img_all = []\n",
    "    for datapath in datapaths:\n",
    "        imgpaths = glob(datapath)\n",
    "        img_name = []\n",
    "        dupe_name = []\n",
    "        for imgpath in imgpaths:\n",
    "            if os.path.basename(imgpath) in img_all:\n",
    "                dupe_name.append(imgpath)\n",
    "                continue\n",
    "            no_cat = True\n",
    "            for cat in categories:\n",
    "                if cat in os.path.dirname(imgpath):\n",
    "                    img_dict[cat].append(imgpath)\n",
    "                    no_cat = False\n",
    "            img_name.append(os.path.basename(imgpath))\n",
    "            \n",
    "        dupe_all += dupe_name\n",
    "        img_all += img_name\n",
    "        \n",
    "    return img_dict,img_all,dupe_all\n",
    "\n",
    "def split_train_test(datapaths:list,categories:list,train_ratio:float,dataset:str='both'):\n",
    "    train_dir = './data/training'\n",
    "    val_dir = './data/validation'\n",
    "    \n",
    "    # categories.append('NG')\n",
    "    for cat in categories:\n",
    "        train_path = os.path.join(train_dir,cat)\n",
    "        val_path = os.path.join(val_dir,cat)\n",
    "        os.makedirs(train_path,exist_ok=True)\n",
    "        os.makedirs(val_path,exist_ok=True)\n",
    "        \n",
    "    img_dict,img_all,dupe_all = consolidate_data(datapaths,categories)\n",
    "    # print(len(img_all))\n",
    "    l = 0\n",
    "    for k in img_dict.keys():\n",
    "        # print(len(img_dict[k]))\n",
    "        l += len(img_dict[k])\n",
    "    # print(l)\n",
    "    \n",
    "    # print('\\n')\n",
    "    for cat in categories:\n",
    "        cat_img = img_dict[cat]\n",
    "        # print(len(cat_img))\n",
    "        np.random.shuffle(cat_img)\n",
    "        if train_ratio < 1 and dataset == 'both':\n",
    "            train_len = int(len(cat_img) * train_ratio)\n",
    "            train_img = cat_img[:train_len]\n",
    "            val_img = cat_img[train_len:]\n",
    "            # print(len(train_img),len(val_img),len(train_img)+len(val_img))\n",
    "\n",
    "            for imgpath in train_img:\n",
    "                # img = cv2.imread(imgpath)\n",
    "                # cv2.imwrite(os.path.join(train_dir,cat,os.path.basename(imgpath)),img)\n",
    "                shutil.copy(imgpath,os.path.join(train_dir,cat,os.path.basename(imgpath)))\n",
    "                pass\n",
    "\n",
    "            for imgpath in val_img:\n",
    "                # img = cv2.imread(imgpath)\n",
    "                # cv2.imwrite(os.path.join(val_dir,cat,os.path.basename(imgpath)),img)\n",
    "                shutil.copy(imgpath,os.path.join(val_dir,cat,os.path.basename(imgpath)))\n",
    "                pass\n",
    "        elif train_ratio == 1 and (dataset == 'validate' or dataset == 'training'):\n",
    "            if dataset == 'train':\n",
    "                for imgpath in cat_img:\n",
    "                    # img = cv2.imread(imgpath)\n",
    "                    # if not cv2.imwrite(os.path.join(train_dir,cat,os.path.basename(imgpath)),img):\n",
    "                    #     print(\"not saved\")\n",
    "                    shutil.copy(imgpath,os.path.join(train_dir,cat,os.path.basename(imgpath)))\n",
    "            else:\n",
    "                for imgpath in cat_img:\n",
    "                    # img = cv2.imread(imgpath)\n",
    "                    # if not cv2.imwrite(os.path.join(val_dir,cat,os.path.basename(imgpath)),img):\n",
    "                    #     print(\"not saved\")\n",
    "                    shutil.copy(imgpath,os.path.join(val_dir,cat,os.path.basename(imgpath)))\n",
    "        else:\n",
    "            raise Exception(\"train ratio and dataset not tally\")\n",
    "            \n",
    "    return img_all,dupe_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62e29c-9a95-4dd0-a100-8cb29ad346ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datapaths = [\n",
    "    '../train_8c_crop_4comp/*/*.png'\n",
    "]\n",
    "\n",
    "categories = cats\n",
    "\n",
    "img_dict,img_all,dupe_all = consolidate_data(datapaths,categories)\n",
    "img_all,dupe_all = split_train_test(datapaths,categories,0.8,dataset='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f8250-4355-4520-8fa9-971fb44af066",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb20d2-d702-46e9-8be1-4a666f3ab0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "class_to_label_dict = {\n",
    "    'Clean_ins': 0,\n",
    "    'DATI': 1,\n",
    "    'OffTrkWrite': 2,\n",
    "    'Scratch_radial': 3,\n",
    "    'Ser_deg_ins': 4,\n",
    "    'Ser_instability': 5,\n",
    "    'Spacing_MD_PW_SerDeg': 6,\n",
    "    'Wr_related_1sct': 7\n",
    "}\n",
    "\n",
    "label_to_class_dict = {\n",
    "    0: 'Clean_ins',\n",
    "    1: 'DATI',\n",
    "    2: 'OffTrkWrite',\n",
    "    3: 'Scratch_radial',\n",
    "    4: 'Ser_deg_ins',\n",
    "    5: 'Ser_instability',\n",
    "    6: 'Spacing_MD_PW_SerDeg',\n",
    "    7: 'Wr_related_1sct'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103a18f-877d-4252-9fc5-72a8bb678c3c",
   "metadata": {},
   "source": [
    "## Define a CNN architecture with 4 Conv layer + 4 FCN layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51a0bc-def6-4532-ab92-f6ffbf1c6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitflipModel(nn.Module):\n",
    "    def __init__(self, chnum_in):\n",
    "        super(BitflipModel, self).__init__()\n",
    "        self.chnum_in = chnum_in\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.chnum_in, 8, (3,3),stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, (3,3), stride=1,padding='same'),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, (3,3),stride=1, padding='valid'),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, (3,3),stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(8,32)\n",
    "        self.linear2 = nn.Linear(32,64)\n",
    "        self.linear3 = nn.Linear(64,16)\n",
    "        self.linear4 = nn.Linear(16,8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7975209b-e67d-4407-bc96-55462aafb174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "BitflipModel                             [1, 8]                    --\n",
      "├─Sequential: 1-1                        [1, 8, 113, 224]          --\n",
      "│    └─Conv2d: 2-1                       [1, 8, 113, 224]          224\n",
      "│    └─BatchNorm2d: 2-2                  [1, 8, 113, 224]          16\n",
      "│    └─ReLU: 2-3                         [1, 8, 113, 224]          --\n",
      "├─Sequential: 1-2                        [1, 32, 113, 224]         --\n",
      "│    └─Conv2d: 2-4                       [1, 32, 113, 224]         2,336\n",
      "│    └─BatchNorm2d: 2-5                  [1, 32, 113, 224]         64\n",
      "│    └─ReLU: 2-6                         [1, 32, 113, 224]         --\n",
      "├─MaxPool2d: 1-3                         [1, 32, 56, 112]          --\n",
      "├─Sequential: 1-4                        [1, 16, 54, 110]          --\n",
      "│    └─Conv2d: 2-7                       [1, 16, 54, 110]          4,624\n",
      "│    └─BatchNorm2d: 2-8                  [1, 16, 54, 110]          32\n",
      "│    └─ReLU: 2-9                         [1, 16, 54, 110]          --\n",
      "│    └─Dropout2d: 2-10                   [1, 16, 54, 110]          --\n",
      "├─MaxPool2d: 1-5                         [1, 16, 27, 55]           --\n",
      "├─Sequential: 1-6                        [1, 8, 27, 55]            --\n",
      "│    └─Conv2d: 2-11                      [1, 8, 27, 55]            1,160\n",
      "│    └─BatchNorm2d: 2-12                 [1, 8, 27, 55]            16\n",
      "│    └─ReLU: 2-13                        [1, 8, 27, 55]            --\n",
      "├─MaxPool2d: 1-7                         [1, 8, 13, 27]            --\n",
      "├─AdaptiveAvgPool2d: 1-8                 [1, 8, 1, 1]              --\n",
      "├─Flatten: 1-9                           [1, 8]                    --\n",
      "├─Linear: 1-10                           [1, 32]                   288\n",
      "├─Linear: 1-11                           [1, 64]                   2,112\n",
      "├─Linear: 1-12                           [1, 16]                   1,040\n",
      "├─Linear: 1-13                           [1, 8]                    136\n",
      "==========================================================================================\n",
      "Total params: 12,048\n",
      "Trainable params: 12,048\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 93.99\n",
      "==========================================================================================\n",
      "Input size (MB): 0.30\n",
      "Forward/backward pass size (MB): 17.91\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 18.26\n",
      "==========================================================================================\n",
      "Wrong model size: 46478.271484375\n",
      "Actual model size: 48192\n",
      "Actual competition score: 89.73109561752987\n"
     ]
    }
   ],
   "source": [
    "model = BitflipModel(3)\n",
    "model.to(device)\n",
    "print(summary(model,input_size=(1,3,113,224),device=device))\n",
    "print(f\"Wrong model size: {get_model_size(model)*1000**2}\")\n",
    "print(f\"Actual model size: {12048*4}\")\n",
    "print(f\"Actual competition score: {0.8972*100 + 534.72/(12048*4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c7d29-c145-4e0d-b36b-4d0c812849af",
   "metadata": {},
   "source": [
    "## Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaac2c-65b4-4fa4-a7f7-62b52a0e6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_path, transform):\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        for k in class_to_label_dict.keys():\n",
    "            if k in self.image_path[idx]:\n",
    "                label = class_to_label_dict[k]\n",
    "            \n",
    "        return (image,label)\n",
    "        # return (img,roi,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad541753-2558-4a16-927b-8aa66241f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    'train': transforms.Compose(\n",
    "        [transforms.RandomChoice([\n",
    "             transforms.Compose([\n",
    "                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                 transforms.RandomHorizontalFlip(p=0.5)\n",
    "             ]),\n",
    "             transforms.RandomVerticalFlip(p=0.5),\n",
    "             transforms.RandomHorizontalFlip(p=0.5)\n",
    "         ]),\n",
    "         transforms.RandomAffine(degrees=0,translate=(0,0.1)),\n",
    "         transforms.Resize((113,224)), #145,435, #128,336\n",
    "         transforms.CenterCrop((113,210)),\n",
    "         transforms.ToTensor()\n",
    "         ]),\n",
    "    'test': transforms.Compose(\n",
    "        [transforms.Resize((113,224)),\n",
    "         transforms.CenterCrop((113,210)),\n",
    "         transforms.ToTensor()\n",
    "         ])}\n",
    "\n",
    "def load_data(data_folder, batch_size, phase='train', train_val_split=True, train_ratio=.8):\n",
    "\n",
    "    data = CustomDataset(data_folder,transform_dict[phase])\n",
    "    \n",
    "    if phase == 'train':\n",
    "        if train_val_split:\n",
    "            train_size = int(train_ratio * len(data))\n",
    "            test_size = len(data) - train_size\n",
    "            data_train, data_val = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "            train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "            val_loader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                                                num_workers=4)\n",
    "            return [train_loader, val_loader]\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "            return train_loader\n",
    "    else: \n",
    "        test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                                                    num_workers=4)\n",
    "        return test_loader\n",
    "\n",
    "num_batch = 8\n",
    "directory_train = glob('./data/training/*/*.png')\n",
    "directory_test = glob('./data/validation/*/*.png') \n",
    "loader_train = load_data(directory_train, batch_size=num_batch, phase='train', train_val_split=False, train_ratio=0.5)\n",
    "loader_test = load_data(directory_test, batch_size=num_batch, phase='test', train_val_split=False, train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f34c1-9f06-4f58-a9e7-7851e06b56ab",
   "metadata": {},
   "source": [
    "## Calculate class weight of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd7af2-15fc-4a6d-8edf-938649e75dd6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "yw = []\n",
    "for d in directory_train:\n",
    "    for k in class_to_label_dict.keys():\n",
    "        if k in d:\n",
    "            label = class_to_label_dict[k]\n",
    "    yw.append(label)\n",
    "    \n",
    "class_weight = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(yw),y=yw)\n",
    "class_weight = torch.tensor(class_weight).float().to(device)\n",
    "class_weight\n",
    "# pos_weight = class_weight[1]/class_weight[0]\n",
    "# pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb58620-0c3a-4bca-8991-f28cce8b5e65",
   "metadata": {},
   "source": [
    "## Define optimizer, loss function, epochs and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1299db7-448a-4a07-9bcb-1140799f9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "schedulerplat = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.9,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e02dcf-3fd0-482e-86f3-b96e1dd6e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    _,y_prob = torch.max(y_prob,1)\n",
    "    return (y_prob == y_true).float().mean().item()\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # model.apply(set_bn_eval)\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    loss_acc = 0\n",
    "    correct_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # y = y.float()\n",
    "        pred = model(X)\n",
    "        # pred = pred.squeeze(1)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(loss)\n",
    "        correct = get_accuracy(y,pred)\n",
    "        loss_acc += loss.item()\n",
    "        correct_acc += correct\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(len(X))\n",
    "        if batch % 50 == 0:\n",
    "            # print(batch,len(X))\n",
    "            loss, correct,current = loss.item(), correct,batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  correct: {correct:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    loss_acc /= (np.ceil(size/num_batch))\n",
    "    correct_acc /= (np.ceil(size/num_batch))\n",
    "    \n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct_acc):>0.1f}%, Avg loss: {loss_acc:>8f} \\n\")\n",
    "            \n",
    "    return loss_acc\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.type(torch.LongTensor).to(device)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # y = y.float()\n",
    "            pred = model(X)\n",
    "            # pred = pred.squeeze(1)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            correct += get_accuracy(y,pred)\n",
    "            test_loss += loss\n",
    "            # print(correct)\n",
    "\n",
    "            \n",
    "    test_loss /= (np.ceil(size/num_batch))\n",
    "    correct /= (np.ceil(size/num_batch))\n",
    "    # print(correct)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc05775-c5b9-49d3-8244-daa9ca513043",
   "metadata": {},
   "source": [
    "## Run model training, save model weight to **model_save** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e6456-986b-436e-8a87-e787d221b863",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_acc = []\n",
    "test_loss_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(loader_train, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(loader_test, model, loss_fn)\n",
    "    train_loss_acc.append(train_loss)\n",
    "    test_loss_acc.append(test_loss)\n",
    "    if t >= 170:\n",
    "        schedulerplat.step(test_loss)\n",
    "    os.makedirs('./model_save',exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'./model_save/epoch_{t}.pth')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40b80c-f79d-4f0a-999e-e688f6c93336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_acc)\n",
    "plt.plot(test_loss_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6c4ad-15d5-459e-88f9-1fbea51f0272",
   "metadata": {},
   "source": [
    "# Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495c966-a15a-4f41-9980-655b32c6f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./model/epoch_407.pth',map_location=device) #335,355 382\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def predModel(x,verbose=False):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "    return pred\n",
    "\n",
    "def arr2tensor(img_arr):\n",
    "    img = Image.fromarray(img_arr)\n",
    "    img = transform_dict['test'](img)\n",
    "    # mean, std = torch.mean(img), torch.std(img)\n",
    "    # img = (img - mean)/std\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def tensor2arr(img_tensor):\n",
    "    # img_tensor = invTrans(img_tensor)\n",
    "    img_tensor = (img_tensor[0,...].cpu().detach().permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd339569-eb3e-4641-901e-d72e405fe079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "imgpaths = glob(f'./data/test_resized/**.png') #VL8_Far641_sd_sp\n",
    "np.random.shuffle(imgpaths)\n",
    "imgpath = imgpaths[0]\n",
    "print(imgpath)\n",
    "\n",
    "img = cv2.imread(imgpath)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cv2.resize(img,(224,113)))\n",
    "plt.show()\n",
    "\n",
    "img = Image.fromarray(img)\n",
    "img = transforms.Resize((113,224))(img)\n",
    "img = transforms.CenterCrop((113,210))(img)\n",
    "img = transforms.ToTensor()(img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)\n",
    "\n",
    "pred = predModel(img)\n",
    "pred = nn.Softmax(dim=1)(pred)\n",
    "pred = torch.argmax(pred).item()\n",
    "class_type = label_to_class_dict[pred]\n",
    "print(pred,class_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ae9a9-99f7-4036-86f3-0ac50116d39b",
   "metadata": {},
   "source": [
    "# Generate test result CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306a1dc-e3be-4272-9e63-1c2bc7faf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['imgpath','pred'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5b695-d4bf-4c59-b135-60797215d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgpaths = glob(f'./data/test_resized/*.png')\n",
    "\n",
    "for i,imgpath in enumerate(imgpaths):\n",
    "    \n",
    "    img = Image.open(imgpath).convert('RGB')\n",
    "    img = transforms.Resize((113,224))(img)\n",
    "    img = transforms.CenterCrop((113,210))(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    pred = predModel(img)\n",
    "    pred = nn.Softmax(dim=1)(pred)\n",
    "    pred = torch.argmax(pred).item()\n",
    "    class_type = label_to_class_dict[pred]\n",
    "    # print(pred,truth,class_type,cat,imgpath)\n",
    "    df.loc[i,'imgpath'] = os.path.basename(imgpath)\n",
    "    df.loc[i,'pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c738dfb-3837-4e96-8fe2-146074217aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "df.to_csv('./submission.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0387d-5c97-4a4d-a6ee-74c3264b1587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-new]",
   "language": "python",
   "name": "conda-env-pytorch-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
